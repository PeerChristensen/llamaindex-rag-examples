{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion\n",
    "\n",
    "The following is an example of RAG-Fusion based of the following github repo [RAG-Fusion by Raudaschl](https://github.com/Raudaschl/rag-fusion) and this blog from LlamaIndex: [Building an Advanced Fusion Retriever from Scratch](https://docs.llamaindex.ai/en/stable/examples/low_level/fusion_retriever/?h=fusion)\n",
    "\n",
    "The example performs the following steps:\n",
    "\n",
    "- Query expansion\n",
    "- Vector Search Using Multiple Retrievers\n",
    "- Fusion of the results using *Reciprocal Rank Fusion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "- Import the necessary libraries\n",
    "- Load environment variables\n",
    "- Fetch knowledge base from Malazan Wiki\n",
    "- Create `VectorStoreIndex` using the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import get_malazan_pages, generate_vector_index, create_and_save_md_files\n",
    "from llama_index.retrievers.bm25 import BM25Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following to a `.env` file in the root of the project if not already there.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_malazan_pages(articles=[\"Anomander Rake\", \"Tayschrenn\", \"Kurald Galain\", \"Warrens\", \"Tattersail\", \"Whiskeyjack\", \"Kruppe\"])\n",
    "create_and_save_md_files(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = generate_vector_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the titles of Anomander Rake?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What type of warren does Tayshrenn use?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion\n",
    "\n",
    "Query expansion is part of the **Pre-retrieval** phase of Advanced RAG.\n",
    "\n",
    "The idea behind Query Expansion is to generate multiple queries from the original query and then use these queries to search for relevant documents. The idea is that by using multiple queries, we can capture more information about the user's intent and hence retrieve more relevant documents.\n",
    "\n",
    "Two techniques to generate multiple queries are:\n",
    "\n",
    "- **Multi-Query**\n",
    "    - Generate multiple questions from the original query using an LLM. Search the vector database using these questions and then fuse the results.\n",
    "- **Generated Answer**\n",
    "    - Generate a hypothetical answer from the original query using an LLM without context. Search the vector database using both the answer and the original query and then fuse the results.\n",
    "    - The idea behind using answers is that the documents that are similar to the answer are likely to be relevant to the query.\n",
    "    - The answer doesn't necessarily have to be a correct answer, it can be a random sentence generated by the LLM.\n",
    "\n",
    "We use \"Multiple Questions\" in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "llm = LlamaOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "generate_queries_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \n",
    "Generate {num_queries} search queries, one on each line, related to the following input query:\n",
    "Query: {query}\n",
    "Queries\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = generate_queries_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = [query] + response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generate_queries(llm, query, num_queries=4)\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run queries against multiple retrievers\n",
    "\n",
    "Now we run the generated queries against two different retrievers and get the results.\n",
    "\n",
    "- Default Vector Search Retriever with cosine similarity\n",
    "- BM25 Retriever using Okabi B25 implementation\n",
    "\n",
    "#### BM25 Retriever\n",
    "\n",
    "The second retriever is a BM25 retriever. BM25 is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query. It is based on the probabilistic information retrieval model. The BM25 function is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "\n",
    "    results_dict = {}\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            query_result = retriever.retrieve(query)            \n",
    "            results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = run_queries(queries, [vector_retriever, bm25_retriever])\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse the results using Reciprocal Rank Fusion\n",
    "\n",
    "Reciprocal Rank Fusion is a simple fusion method that combines the results of multiple retrievers based on the reciprocal rank of the documents. The idea is to give more weight to the documents that appear higher in the ranking of the individual retrievers.\n",
    "\n",
    "Reciprocal Rank Fusion works by calculating the reciprocal rank of each document in the ranking of each retriever and then summing the reciprocal ranks for each document. The documents with higher cumulative reciprocal ranks are considered more relevant and are ranked higher in the final fusion result.\n",
    "\n",
    "Reciprocal Rank is calculated as:\n",
    "`1/(rank+k)`\n",
    "where `k` is a constant that can be used to control the weight of the reciprocal rank. A higher value of `k` gives more weight to the reciprocal rank, while a lower value gives less weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 2) -> List[NodeWithScore]:\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Retriever \n",
    "\n",
    "Finally we create a FusionRetriever using the functions we've defined above and run a query against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "        \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=4\n",
    "        )\n",
    "        results = run_queries(queries, self._retrievers)\n",
    "        print(\"Queries:\")\n",
    "        for query in queries:\n",
    "            print(query)\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        print(\"Final Results:\")\n",
    "        for node_with_score in final_results:\n",
    "            print(node_with_score)\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the `QueryFusionRetriever` from LlamaIndex which does that same behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=4,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `RetrieverQueryEngine` and run a query against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(fusion_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(query).response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
