{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation RAG pipelines\n",
    "\n",
    "A RAG pipeline is a combination of two components: a retriever and a generator. The retriever is responsible for finding the most relevant documents to the input question, and the generator is responsible for generating the answer based on the retrieved documents. For this reason it makes sense to define metrics for each of these components separately:\n",
    "\n",
    "- Retriever metrics:\n",
    "  - **Context precision**: Ground truth relevant items are ranked at the top\n",
    "  - **Context recall**: How does the context align with the ground truth\n",
    "  - **Context relevancy**: How relevant is the retrieved context to the question\n",
    "  - **Context entity recall**: How much of the ground truth is included in the context\n",
    "- Generator metrics:\n",
    "  - **Answer relevancy**: How relevant is the generated answer to the question\n",
    "  - **Answer correctness**: Combined factual similarity and semantic similarity between the answer and the ground truth\n",
    "  - **Faithfulness**: How many factual claims in the answer can be inferred directly from the context\n",
    "\n",
    "These metrics are defined by the [**Ragas**](https://docs.ragas.io/en/latest/concepts/metrics/index.html) framework, which is a Python library for evaluating RAG pipelines. Another framework used for evaluation is the RAG Evaluation Toolkit or [**RAGET**](https://docs.giskard.ai/en/stable/open_source/testset_generation/index.html).\n",
    "\n",
    "We will also look at how to generate synthetic testsets for evaluating RAG, since hand-made testsets is a luxury that can take a lot of effort to crate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"giskard[llm]\" -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragas pandas llama-index-readers-wikipedia unstructured[md]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your `.env` file contains the following variables:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<your_key>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True, verbose=True)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "\n",
    "from util.helpers import create_and_save_wiki_md_files, get_wiki_pages\n",
    "\n",
    "# RAGET\n",
    "import giskard.llm\n",
    "from giskard.llm.client.openai import OpenAIClient\n",
    "from giskard.rag import generate_testset, evaluate, KnowledgeBase, QATestset\n",
    "from giskard.rag.question_generators import (\n",
    "    distracting_questions,\n",
    "    double_questions,\n",
    "    simple_questions,\n",
    "    situational_questions,\n",
    "    complex_questions,\n",
    ")\n",
    "from giskard.rag.metrics.ragas_metrics import (\n",
    "    ragas_context_recall, \n",
    "    ragas_context_precision,\n",
    "    ragas_faithfulness,\n",
    ")\n",
    "\n",
    "# Llama Index\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.query_engine import (\n",
    "    FLAREInstructQueryEngine, \n",
    "    BaseQueryEngine\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# RAGAs\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and save documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_wiki_pages(articles=[\"Albert Einstein\"])\n",
    "docs_path = \"./data/docs/eval\"\n",
    "create_and_save_wiki_md_files(pages, path=docs_path + \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a testset\n",
    "\n",
    "When evaluating the performance of ML its almost always necessary to have a testset of some kind. In the case of RAG, you need a testset consisting of *queries* together with their respective *answers* and *contexts*. The context is the document that the retriever should retrieve, and the answer is the expected output of the generator.\n",
    "\n",
    "In some cases you might already have access a testset. For example if you're working on a pipeline to generate automatic answers to support tickets, you might have a set of successfully handled tickets together with some guides or manuals that your support staff should base their answer on. But in many cases it can take a lot of work to create a viable testset. In those cases it can make sense to create a syntehtic testset using LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RAGAs\n",
    "\n",
    "**OBS**: Currently there's a bug in Ragas which often causes a deadlock when generating the testset. I recommend skipping for now or check the issue [*here*](https://github.com/explodinggradients/ragas/issues/833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_documents = DirectoryLoader(docs_path).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", api_key=OPENAI_API_KEY)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4-turbo\", api_key=OPENAI_API_KEY)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "ragas_testset = generator.generate_with_langchain_docs(langchain_documents, test_size=1, with_debugging_logs=True, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}, is_async=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RAGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "giskard.llm.set_llm_api(\"openai\")\n",
    "oc = OpenAIClient(model=\"gpt-4-turbo\")\n",
    "giskard.llm.set_default_client(oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaindex_documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "text_nodes = splitter(llamaindex_documents)\n",
    "\n",
    "pd_dataframe = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "\n",
    "knowledge_base = KnowledgeBase(data=pd_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the test set. We generate 5 different types of questions, which each are targeted at testing specific components of the RAG pipeline:\n",
    "\n",
    "- **Simple**: Simple questions generated from an excerpt of the knowledge base \n",
    "    - targeted at evaluating *generation* and *retrieval*\n",
    "- **Complex**: Questions made more complex by paraphrasing \n",
    "    - targeted at evaluating *generation*\n",
    "- **Situational**: Questions including user context to evaluate the ability of the generation to produce relevant answer according to the context \n",
    "    - targeted at evaluating *generation*\n",
    "- **Double**: Questions with two distinct parts \n",
    "    - targeted at evaluating *generation* and *rewriting*\n",
    "- **Distracting**: Questions made to confuse the retrieval part of the RAG with a distracting element from the knowledge base but irrelevant to the question \n",
    "    - targeted at evaluating *generation*, *retrieval* and *rewriting*\n",
    "\n",
    "Other question types include:\n",
    "- **Conversational**: Questions made as part of a conversation, first message describe the context of the question that is ask in the last message\n",
    "    - targeted at evaluating *rewriting* and *routing*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raget_testset = generate_testset(\n",
    "    knowledge_base=knowledge_base,\n",
    "    num_questions=20,\n",
    "    agent_description=\"A chatbot answering questions about Albert Einstein\",\n",
    "    question_generators=[simple_questions, complex_questions, situational_questions, double_questions, distracting_questions],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./data/eval\"\n",
    "if not os.path.exists(dir):\n",
    "    print(\"Creating directory: \", dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "path = f\"{dir}/einstein_testset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raget_testset.save(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raget_testset = QATestset.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raget_testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a RAG pipeline\n",
    "\n",
    "Now, given our generated testset, we can evaluate a RAG pipeline. In this examples we use RAGET (with RAGAs metrics) to evaluate two different pipelines:\n",
    "\n",
    "- **Baseline RAG**\n",
    "- **Advanced RAG with FLARE**\n",
    "\n",
    "First we create our query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=llamaindex_documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple baseline RAG pipeline\n",
    "\n",
    "We create a simple RAG setup with the default values from LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FLARE query engine\n",
    "\n",
    "We create a FLARE query engine using the `FLAREInstructQueryEngine` model from LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_query_engine = FLAREInstructQueryEngine(\n",
    "    query_engine=index.as_query_engine(), \n",
    "    max_iterations=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `answer_fn` for RAGET to use in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_fn(question: str, query_engine: BaseQueryEngine) -> str:\n",
    "    answer = query_engine.query(question)\n",
    "    \n",
    "    return str(answer)\n",
    "\n",
    "def base_answer_fn(question: str, history=None) -> str:\n",
    "    return answer_fn(question, base_query_engine)\n",
    "\n",
    "def flare_answer_fn(question: str, history=None) -> str:\n",
    "    return answer_fn(question, flare_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "We use RAGET's `evaluate` function to evaluate the two pipelines using the metrics: Answer Correctness (this is default in the `evaluate` fn), Faithfulness, Context Precision, Context Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_report = evaluate(\n",
    "    base_answer_fn,\n",
    "    testset=raget_testset,\n",
    "    knowledge_base=knowledge_base,\n",
    "    metrics=[\n",
    "        ragas_faithfulness,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_report.save(f\"{dir}/base_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"file://{os.getcwd()}{dir[1:]}/base_report/report.html\"\n",
    "print(url)\n",
    "webbrowser.open(url=url, new=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_report = evaluate(\n",
    "    flare_answer_fn,\n",
    "    testset=raget_testset,\n",
    "    knowledge_base=knowledge_base,\n",
    "    metrics=[\n",
    "        ragas_faithfulness,\n",
    "        ragas_context_recall,\n",
    "        ragas_context_precision,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_report.save(f\"{dir}/flare_report\")\n",
    "url = f\"file://{os.getcwd()}{dir[1:]}/flare_report/report.html\"\n",
    "print(url)\n",
    "webbrowser.open(url=url, new=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
