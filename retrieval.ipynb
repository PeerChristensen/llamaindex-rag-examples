{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "In baseline RAG, retrieval is usually done simply by using vector search. However, as the field has developed, researchers have discovered multiple ways to enhance the retrieval process. This notebook will cover the following retrieval methods: \n",
    "\n",
    "- **Iterative Retrieval** - Use the reasoning capabilities of LLMs to perform iterative retrieval-generation cycles until passing an evaluation step.\n",
    "- **Recursive-Retrieval** - Search for smaller documents and use chunk references or metadata references to retrieve the full document.\n",
    "- **Generator-Enhanced Retrieval** - Using a LLM to predict when and what to retrieve across generation with a process called [**F**orward-**L**ooking **A**ctive **RE**trieval augmented\n",
    "generation (FLARE)](https://arxiv.org/pdf/2305.06983)\n",
    "- **GraphRAG** - Using existing or LLM-generated knowledge graphs to enhance retrieval by harnessing the relationships between entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipython-ngql pyvis llama-index-readers-wikipedia llama-index-readers-papers llama-index-readers-web llama-index-graph-stores-nebula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from util.helpers import (\n",
    "    get_malazan_pages,\n",
    "    create_and_save_md_files,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    KnowledgeGraphIndex,\n",
    "    PromptTemplate,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.query_engine import (\n",
    "    FLAREInstructQueryEngine,\n",
    "    RetrieverQueryEngine,\n",
    "    RetryQueryEngine,\n",
    "    RetryGuidelineQueryEngine,\n",
    "    KnowledgeGraphQueryEngine,\n",
    ")\n",
    "from llama_index.readers.papers import ArxivReader\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    GuidelineEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following to a `.env` file in the root of the project if not already there.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read papers from arXiv as documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ArxivReader()\n",
    "\n",
    "papers = [\"2404.10981\", \"2305.06983\", \"2312.10997\"]\n",
    "papers_dir = \"./data/docs/arxiv\"\n",
    "\n",
    "arxiv_res = [reader.load_papers_and_abstracts(search_query=f\"id:{paper}\", max_results=1, papers_dir=papers_dir) for paper in papers]\n",
    "arxiv_documents = [doc for sublist in [d for (d, _) in arxiv_res] for doc in sublist]\n",
    "abstracts = [a[0] for (_, a) in arxiv_res]\n",
    "arxiv_index = VectorStoreIndex.from_documents(documents=arxiv_documents, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read articles from Malazan Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_malazan_pages(articles=[\"Anomander Rake\"])\n",
    "docs_path = \"./data/docs/graph_rag\"\n",
    "create_and_save_md_files(pages, path=docs_path + \"/\")\n",
    "\n",
    "malazan_documents = SimpleDirectoryReader(input_dir=docs_path).load_data()\n",
    "malazan_index = VectorStoreIndex.from_documents(\n",
    "    malazan_documents,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Wiki page about Vincent Van Gogh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = WikipediaReader()\n",
    "wiki_documents = reader.load_data(pages=[\"Vincent Van Gogh\"])\n",
    "wiki_index = VectorStoreIndex.from_documents(documents=wiki_documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Retrieval\n",
    "In **Iterative Retrieval**, the key part of the process is the evaluator or *judge* that helps self-correct the retrieval process. \n",
    "\n",
    "The pipeline performs the following steps:\n",
    "1. first queries the base query engine, then\n",
    "2. use the evaluator to decided if the response passes.\n",
    "3. If the response passes, then return response,\n",
    "4. Otherwise, transform the original query with the evaluation result (query, response, and feedback) into a new query,\n",
    "5. Repeat up to max_retries\n",
    "\n",
    "There's different types of evaluators that can be used, depending on the use-case. Sometimes it might be useful to evaluate the answer specifically, or the context. Other times you might need to create some guidelines for the LLM to use in order to evaluate whether to perform another retrieval step or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt of RelevancyEvaluator\n",
    "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n",
    "    \"Your task is to evaluate if the response for the query \\\n",
    "    is in line with the context information provided.\\n\"\n",
    "    \"You have two options to answer. Either YES/ NO.\\n\"\n",
    "    \"Answer - YES, if the response for the query \\\n",
    "    is in line with context information otherwise NO.\\n\"\n",
    "    \"Query and Response: \\n {query_str}\\n\"\n",
    "    \"Context: \\n {context_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"We want to understand if the following query and response is\"\n",
    "    \"in line with the context information: \\n {query_str}\\n\"\n",
    "    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"If the existing answer was already YES, still answer YES. \"\n",
    "    \"If the information is present in the new context, answer YES. \"\n",
    "    \"Otherwise answer NO.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RelevancyEvaluator(llm=llm)\n",
    "query_engine = RetryQueryEngine(\n",
    "    query_engine=arxiv_index.as_query_engine(), evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GuidelineEvaluator(\n",
    "    llm=llm, guidelines=\"List elements of the answer as bullets\"\n",
    ")\n",
    "query_engine = RetryGuidelineQueryEngine(\n",
    "    query_engine=arxiv_index.as_query_engine(),\n",
    "    resynthesize_query=True,\n",
    "    guideline_evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Advanced Retrieval Augmented Generation?\"\n",
    "response = query_engine.query(str_or_query_bundle=query)\n",
    "display(Markdown(f\"Query: {query}\\n\\nResponse:\\n\\n {response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive-Retrieval\n",
    "\n",
    "In **Recursive-Retrieval**, we search to relevant documents using references between smaller documents and their related larger documents. This enables us to optimize the narrowing the search space for the first retrieval, and then recursively increasing the context using the references.\n",
    "\n",
    "We distinguish between two types of recursive-retrieval:\n",
    "1. **Chunk-References** - References between smaller and larger documents.\n",
    "2. **Metadata-References** - References between metadata like summaries or generated questions to documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "base_nodes = node_parser.get_nodes_from_documents(documents=wiki_documents, show_progress=True)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_chunk_sizes = [62, 124, 256, 512]\n",
    "sub_node_parsers = [\n",
    "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes: list[IndexNode] = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the effectiveness of chunk references go to \"**Test recursive-retrieval engine**\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    SummaryExtractor(summaries=[\"self\"], llm=OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\"), show_progress=True),\n",
    "    QuestionsAnsweredExtractor(questions=5, llm=OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\"), show_progress=True),\n",
    "]\n",
    "# run metadata extractor across base nodes, get back dictionaries\n",
    "node_to_metadata = {}\n",
    "for extractor in extractors:\n",
    "    metadata_dicts = extractor.extract(base_nodes)\n",
    "    for node, metadata in zip(base_nodes, metadata_dicts):\n",
    "        if node.node_id not in node_to_metadata:\n",
    "            node_to_metadata[node.node_id] = metadata\n",
    "        else:\n",
    "            node_to_metadata[node.node_id].update(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_dicts(path, data):\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "\n",
    "def load_metadata_dicts(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"./data/retrieval\"\n",
    "if not os.path.exists(path):\n",
    "    print(\"Creating directory: \", path)\n",
    "    os.makedirs(path)\n",
    "\n",
    "save_metadata_dicts(path + \"/metadata_dicts.json\", node_to_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dicts = load_metadata_dicts(\"./data/retrieval/metadata_dicts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = copy.deepcopy(base_nodes)\n",
    "for node_id, metadata in node_to_metadata.items():\n",
    "    for val in metadata.values():\n",
    "        all_nodes.append(IndexNode(text=val, index_id=node_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test recursive-retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "recursive_index = VectorStoreIndex(all_nodes, embed_model=embed_model, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": recursive_index.as_retriever()},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")\n",
    "recursive_query_engine = RetrieverQueryEngine.from_args(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\"Where did Vincent Van Gogh live during his life?\")\n",
    "\n",
    "for node in nodes:\n",
    "    display(Markdown(f\"### {node.node_id}\"))\n",
    "    display(Markdown(f\"{node.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = recursive_query_engine.query(\"Where did Vincent Van Gogh live during his life?\")\n",
    "\n",
    "display(Markdown(f'{response}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator-Enhanced Retrieval\n",
    "\n",
    "The following example will demonstrate the **F**orward **L**ooking **A**ctive **RE**trieval augmented generation (FLARE). This method actively uses the LLM to help with retrieval by generating retrieval instructions. FLARE has the following methodology:\n",
    "\n",
    "- **Multiple retrievals**: FLARE uses multiple retrievals based on queries created by the LLM.\n",
    "- **When to retrieve**: Uses LLM to decide when to retrieve.\n",
    "- **What to retrieve**: Uses LLM to decide what to retrieve.\n",
    "- **Iterative**: Iteratively expands the answer by \"looking ahead\" to decide what to retrieve next.\n",
    "\n",
    "The following is the instruct prompt used in each iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Skill 1. Use the Search API to look up relevant information by writing [Search(query)]\" where \"query\" is the search query you want to look up.\n",
    "For example:\n",
    "\n",
    "Query: But what are the risks during production of nanomaterials?\n",
    "Answer: [Search(What are some nanomaterial production risks?)]\n",
    "\n",
    "Query: The colors on the flag of Ghana have the following meanings.\n",
    "Answer: Red is for [Search(What is the meaning of Ghana's flag being red?)], green for forests, and gold for mineral wealth.\n",
    "\n",
    "Query: What did the author do during his time in college?\n",
    "Answer: The author took classes in [Search(What classes did the author take in college?)].\n",
    "------------------------------------------------------------\n",
    "Skill 2. Solve more complex generation tasks by thinking step by step. For example:\n",
    "\n",
    "Query: Give a summary of the author's life and career.\n",
    "Answer: The author was born in 1990. Growing up, he [Search(What did the author do during his childhood?)].\n",
    "\n",
    "Query: Can you write a summary of the Great Gatsby.\n",
    "Answer: The Great Gatsby is a novel written by F. Scott Fitzgerald. It is about [Search(What is the Great Gatsby about?)].\n",
    "------------------------------------------------------------\n",
    "Now given the following task, and the stub of an existing answer, generate the next portion of the answer. \n",
    "You may use the Search API \"[Search(query)]\" whenever possible.\n",
    "If the answer is complete and no longer contains any \"[Search(query)]\" tags, write \"done\" to finish the task.\n",
    "Do not write \"done\" if the answer still contains \"[Search(query)]\" tags.\n",
    "Do not make up answers. It is better to generate one \"[Search(query)]\" tag and stop generation than to fill in the answer with made up information with no \"[Search(query)]\" tags or multiple \"[Search(query)]\" tags that assume a structure in the answer.\n",
    "Try to limit generation to one sentence if possible.\n",
    "\n",
    "\n",
    "Query: {query_str}\n",
    "Existing Answer: {existing_answer}\n",
    "Answer: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt uses *Few-shot* prompting to define three skills:\n",
    "- **Search API skill**: Gives the LLM the ability to create *query tasks* for information in the next iteration.\n",
    "- **Step-by-step generation skill**: Gives the LLM the ability answer more complex questions by thinking step by step.\n",
    "- **\"When to stop\" skill**: Check when the answer is complete by looking for uses of the \"*Search API*\".\n",
    "\n",
    "After each iteration all query tasks are parsed and the queries are used to retrieve information. The retrieved information is then parsed and then used to \"Look ahead\" to what the next query could be. The following is the *lookahead* prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "An existing 'lookahead response' is given below. The lookahead response contains `[Search(query)]` tags. Some queries have been executed and the response retrieved. The queries and answers are also given below.\n",
    "Also the previous response (the response before the lookahead response) is given below.\n",
    "Given the lookahead template, previous response, and also queries and answers, please 'fill in' the lookahead template with the appropriate answers.\n",
    "\n",
    "NOTE: Please make sure that the final response grammatically follows the previous response + lookahead template. For example, if the previous response is \"New York City has a population of \" and the lookahead template is \"[Search(What is the population of New York City?)]\", then the final response should be \"8.4 million\".\n",
    "\n",
    "NOTE: the lookahead template may not be a complete sentence and may contain trailing/leading commas, etc. Please preserve the original formatting of the lookahead template if possible.\n",
    "\n",
    "NOTE: the exception to the above rule is if the answer to a query is equivalent to \"I don't know\" or \"I don't have an answer\". In this case, modify the lookahead template to indicate that the answer is not known.\n",
    "\n",
    "NOTE: the lookahead template may contain multiple `[Search(query)]` tags and only a subset of these queries have been executed. Do not replace the `[Search(query)]` tags that have not been executed.\n",
    "------------------------------------------------------------\n",
    "Previous Response:\n",
    "\n",
    "Lookahead Template: Red is for [Search(What is the meaning of Ghana's flag being red?)], green for forests, and gold for mineral wealth.\n",
    "\n",
    "Query-Answer Pairs:\n",
    "Query: What is the meaning of Ghana's flag being red?\n",
    "Answer: The red represents the blood of those who died in the country's struggle \\\n",
    "    for independence\n",
    "\n",
    "Filled in Answers: Red is for the blood of those who died in the country's struggle for independence, green for forests, and gold for mineral wealth.\n",
    "------------------------------------------------------------\n",
    "Previous Response: One of the largest cities in the world\n",
    "\n",
    "Lookahead Template: , the city contains a population of [Search(What is the population of New York City?)]\n",
    "\n",
    "Query-Answer Pairs:\n",
    "Query: What is the population of New York City?\n",
    "Answer: The population of New York City is 8.4 million\n",
    "\n",
    "Synthesized Response: , the city contains a population of 8.4 million\n",
    "------------------------------------------------------------\n",
    "Previous Response: the city contains a population of\n",
    "\n",
    "Lookahead Template: [Search(What is the population of New York City?)]\n",
    "\n",
    "Query-Answer Pairs:\n",
    "Query: What is the population of New York City?\n",
    "Answer: The population of New York City is 8.4 million\n",
    "\n",
    "Synthesized Response: 8.4 million\n",
    "------------------------------------------------------------\n",
    "Previous Response:\n",
    "{prev_response}\n",
    "\n",
    "Lookahead Template:\n",
    "{lookahead_response}\n",
    "\n",
    "Query-Answer Pairs:\n",
    "{query_answer_pairs}\n",
    "\n",
    "Synthesized Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_query_engine = FLAREInstructQueryEngine(\n",
    "    llm=llm,\n",
    "    query_engine=malazan_index.as_query_engine(), \n",
    "    max_iterations=5, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = flare_query_engine.query(\"Tell me about Anomander Rake\")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG\n",
    "\n",
    "In this section we will have a look at an example of using knowledge graphs to enhance retrieval. We will show how you can use LLMs to generate our knowledge graphs for us based on arbitrary documents and then use these to search for relevant contexts to supply to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have NebulaGraph running locally, you can use [nebula-up](https://github.com/wey-gu/nebula-up) to start it up.\n",
    "\n",
    "Run the following command in your terminal (On Windows you should use WSL):\n",
    "\n",
    "```bash\n",
    "curl -fsSL nebula-up.siwei.io/install.sh | bash\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your NebulaGraph is started with nebula-up, you can add the following to your .env file:\n",
    "\n",
    "```\n",
    "NEBULA_USER=root\n",
    "NEBULA_PASSWORD=nebula\n",
    "NEBULA_HOST=localhost\n",
    "NEBULA_PORT=9669\n",
    "```\n",
    "\n",
    "Or set with your own configurations for NebulaGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ngql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "GRAPHD_HOST = os.getenv(\"NEBULA_HOST\")\n",
    "GRAPHD_PORT = os.getenv(\"NEBULA_PORT\")\n",
    "NEBULA_PASSWORD = os.getenv(\"NEBULA_PASSWORD\")\n",
    "NEBULA_USER = os.getenv(\"NEBULA_USER\")\n",
    "NEBULA_ADDRESS = f\"{GRAPHD_HOST}:{GRAPHD_PORT}\"\n",
    "os.environ[\"NEBULA_ADDRESS\"] = NEBULA_ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = f\"--address {GRAPHD_HOST} --port {GRAPHD_PORT} --user {NEBULA_USER} --password {NEBULA_PASSWORD}\"\n",
    "%ngql {connection_string}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new knowledge graph (**Graph Space** in Nebula terms) to use for retrieval. We'll create an entity-relationship graph with the following schema:\n",
    "```\n",
    "[entity:tag] - [relationship:edge] -> [entity:tag]\n",
    "```\n",
    "\n",
    "This will allow the LLM to automatically generate our knowledge graph with arbitrary relationships like:\n",
    "```\n",
    "[entity:David] - [relationship:has studied computer science at] -> [entity:Aarhus University]\n",
    "[entity:David] - [relationship:currently works as a software pilot at] -> [entity:Trifork A/S]\n",
    "```\n",
    "\n",
    "We also generate an index for the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS graph_rag(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "%ngql SHOW SPACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ngql\n",
    "USE graph_rag;\n",
    "CREATE TAG IF NOT EXISTS entity(name string);\n",
    "CREATE EDGE IF NOT EXISTS relationship(relationship string);\n",
    "CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%ngql USE graph_rag; CLEAR SPACE graph_rag; # clean graph space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use LlamaIndex's `KnowledgeGraphIndex` to index the graph and use it for retrieval. It uses the following default prompt to generate triplets for the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ```\n",
    "    Some text is provided below. Given the text, extract up to \n",
    "    {max_knowledge_triplets} \n",
    "    knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\n",
    "    ---------------------\n",
    "    Example:\n",
    "    Text: Alice is Bob's mother.\n",
    "    Triplets:\n",
    "    (Alice, is mother of, Bob)\n",
    "    Text: Philz is a coffee shop founded in Berkeley in 1982.\n",
    "    Triplets:\n",
    "    (Philz, is, coffee shop)\n",
    "    (Philz, founded in, Berkeley)\n",
    "    (Philz, founded in, 1982)\n",
    "    ---------------------\n",
    "    Text: {text}\n",
    "    Triplets:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEBULA_ADDRESS\"] = NEBULA_ADDRESS\n",
    "os.environ[\"NEBULA_PASSWORD\"] = NEBULA_PASSWORD\n",
    "os.environ[\"NEBULA_USER\"] = NEBULA_USER\n",
    "\n",
    "space_name = \"graph_rag\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\n",
    "tags = [\"entity\"]\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "\n",
    "Settings.llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=OpenAIEmbeddingModelType.TEXT_EMBED_3_SMALL)\n",
    "Settings.chunk_size = 512\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a little while. Afterwards we save the index to disk so we don't have to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    malazan_documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_index.storage_context.persist(persist_dir='./data/storage_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import load_index_from_storage\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     persist_dir=\"./data/storage_graph\", graph_store=graph_store\n",
    "# )\n",
    "# kg_index = load_index_from_storage(\n",
    "#     storage_context=storage_context,\n",
    "#     max_triplets_per_chunk=10,\n",
    "#     space_name=space_name,\n",
    "#     edge_types=edge_types,\n",
    "#     rel_prop_names=rel_prop_names,\n",
    "#     tags=tags,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = kg_index.as_retriever().retrieve(\"Anomander Rakes friendship with Caladan Brood\")\n",
    "display(Markdown(f'{\"\\n\\nAnomander{name: Anomander}\".join(response[-1].text.split(\"Anomander{name: Anomander}\")[:10])}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index_query_engine = kg_index.as_query_engine(\n",
    "    retriever_mode=\"keyword\",\n",
    "    verbose=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ngql MATCH p=(n)-[e:relationship*1..2]-(m) WHERE id(n) in ['Anomander'] RETURN n.entity.name,e[0].relationship, m.entity.name LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = kg_index_query_engine.query(\"Tell me about Anomander Rakes relationship with the Malazan Empire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f'{response}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL2Cypher\n",
    "\n",
    "It's also possible that we might already have existing knowledge graphs with useful information which hasn't been indexed with embeddings. This would prohibit us from using the vector index to find the right entities/relationships. In this case its possible to use LLMs to automatically generate Cypher queries to retrieve information from the graph based on the query.\n",
    "\n",
    "NL2Cypher is a tool that can be used to generate Cypher queries from natural language queries. We can use it to generate Cypher queries for our knowledge graph.\n",
    "\n",
    "Currently this tool doesn't work super well with the Knowledge Space generated about \"Anomander Rake\", this shows how you might use it with a different knowledge graph.\n",
    "Some fine-tuning might be needed to get the better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT_TMPL = \"\"\"\n",
    "Generate NebulaGraph query from natural language.\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "---\n",
    "{schema}\n",
    "---\n",
    "Note: NebulaGraph speaks a dialect of Cypher, comparing to standard Cypher:\n",
    "\n",
    "1. it uses double equals sign for comparison: `==` rather than `=`\n",
    "2. it needs explicit label specification when referring to node properties, i.e.\n",
    "v is a variable of a node, and we know its label is Foo, v.`foo`.name is correct\n",
    "while v.name is not.\n",
    "\n",
    "For example, see this diff between standard and NebulaGraph Cypher dialect:\n",
    "```diff\n",
    "< MATCH (p:person)-[:directed]->(m:movie) WHERE m.name = 'The Godfather'\n",
    "< RETURN p.name;\n",
    "---\n",
    "> MATCH (p:`person`)-[:directed]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather'\n",
    "> RETURN p.`person`.`name`;\n",
    "```\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "NebulaGraph Cypher dialect query:\n",
    "\"\"\"\n",
    "DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT = PromptTemplate(\n",
    "    DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT_TMPL,\n",
    "    prompt_type=PromptType.TEXT_TO_GRAPH_QUERY,\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery =  KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    llm=OpenAI(api_key=OPENAI_API_KEY),\n",
    "    verbose=True,\n",
    "    graph_query_synthesis_prompt=DEFAULT_NEBULAGRAPH_NL2CYPHER_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_with_nl2graphquery.generate_query(\"Tell me Anomander's relationship with Andarist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_with_nl2graphquery.query(\n",
    "    \"Tell me about Anomander's relationship Andarist\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
