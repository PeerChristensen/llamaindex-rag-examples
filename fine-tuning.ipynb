{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning for RAG\n",
    "\n",
    "Finetuning is a technique that consists of taking a pretrained or \"frozen\" model and adapting it to the current context by training it with a datasat based on the knowledge base that the model needs to answer queries about.\n",
    "\n",
    "Finetuning is sometimes used instead of RAG, but it can also be used in conjunction with RAG to improve the performance of the model and is often what can give the last bit of performance, when build RAG pipelines that are to be used in production.  When you finetune for RAG you have multiple different components which can be finetuned for different tasks:\n",
    "\n",
    "- **Indexing**: Fintetuning the *embedding* model for higher similarity between queries and their relevant documents\n",
    "- **Pre-retrieval**: Finetune LLMs used in *query routing* or *query-rewriting*.\n",
    "- **Retriever**: Finetune LLMs used in *retrieval* like for *iterative*, *recursive* or *generative* retrieval.\n",
    "- **Post-retrieval**: Finetuning your *reranking* model or prompt *compressor*\n",
    "- **Generator**: If you are using a generator model, you can finetune it to better generate the answers to the queries.\n",
    "\n",
    "In many cases, it makes sense to finetune the different components of the RAG pipeline separately, as they are often trained on different datasets and have different objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-finetuning spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import get_malazan_pages, create_and_save_md_files\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs, generate_cohere_reranker_finetuning_dataset\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following to a `.env` file in the root of the project if not already there.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True, verbose=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_malazan_pages([\"Anomander Rake\"])\n",
    "create_and_save_md_files(pages, path=\"./data/docs/finetune/\")\n",
    "documents = SimpleDirectoryReader(\"./data/docs/finetune\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic training data generation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup a quiz/examination.\"\n",
    "    \"Using the provided context, formulate a single question that captures an important fact from the context.\"\n",
    "    \"Restrict the question to the context information provided.\"\n",
    ")\n",
    "\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    question_gen_query=question_gen_query,\n",
    "    llm=llm,\n",
    ")\n",
    "questions = dataset_generator.generate_dataset_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\\n--\".join([question.query for question in questions.examples[5:10]])\n",
    "display(Markdown(f'--{text}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune embeddings\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune reranker\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/finetuning/rerankers/cohere_custom_reranker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune generator\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/finetuning/openai_fine_tuning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
