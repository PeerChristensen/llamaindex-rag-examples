{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-retrieval processing\n",
    "\n",
    "In the \"Post-retrieval\" phase of RAG, the retrieved documents are processed to extract the relevant information. In order to optimize generation. \n",
    "\n",
    "The retrieval phase results in a list of documents. \n",
    "\n",
    "This notebook demonstrates three different techniques for post-retrieval processing:\n",
    "\n",
    "- Reranking\n",
    "- Compression\n",
    "- Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llmlingua llama-index-postprocessor-rankgpt-rerank llama-index-postprocessor-cohere-rerank llama-index-postprocessor-longllmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.postprocessor.longllmlingua import LongLLMLinguaPostprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following to a `.env` file in the root of the project if not already there.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "COHERE_API_KEY=<YOUR_KEY_HERE>\n",
    "```\n",
    "\n",
    "Sign up for Cohere and create one here: [Cohere Dashboard](https://dashboard.cohere.com/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.helpers import get_wiki_pages, create_and_save_wiki_md_files\n",
    "pages = get_wiki_pages([\"Vincent Van Gogh\"])\n",
    "create_and_save_wiki_md_files(pages=pages, path=\"./data/docs/wiki/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/docs/wiki/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 124\n",
    "Settings.chunk_overlap = 10\n",
    "Settings.llm = llm\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Describe the later life of Vincent Van Gogh.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Since vectors are essentially compressions of the meeaning behind some text, there is a loss of information. So what do we do if relevant information is below top_k cutoff for ou retrieval? The simplest solution would be to increase the top_k value, but this would increase the computational cost. Another problem is that LLMs suffer from the \"Lost in the Middle\" phenomenon, where it usually focuses on the extremes of the input prompt. This means that its prudent to have the most relevant information at the top of the list.\n",
    "\n",
    "A solution to this problem is **reranking**. Reranking fundamentally reorders the documents chunks to highlight the most pertinent results first, effectively reducing the overall document pool, severing a dual purpose in information retrieval, acting as both an enhancer and a filter, delivering refined inputs for more precise language model processing.\n",
    "\n",
    "In this example we will see two approaches to reranking:\n",
    "- LLM reranking \n",
    "    - having a language model rerank the documents\n",
    "    - specifically, we will use RankGPT using ChatGPT from OpenAI\n",
    "- Ranking using Cohere Rerank3 - A managed reranking model by Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of using a language model to rerank documents are that it can understand the context of the query and the documents, and can provide a more nuanced ranking.\n",
    "\n",
    "RankGPT uses the following prompt to rank the retrieved documents:\n",
    "```\n",
    "You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\n",
    "\n",
    "I will provide you with {num} passages, each indicated by number identifier []. \n",
    "\n",
    "Rank the passages based on their relevance to query: {query}.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = RankGPTRerank(top_n=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "display(Markdown(f'Nodes:\\n\\n{\"\\n\\n---------------\\n\\n\".join([node.text for node in response.source_nodes])}'))\n",
    "display(Markdown(f'<b>{response}</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere\n",
    "\n",
    "In this example we will use [**Rerank 3**](https://cohere.com/blog/rerank-3), which is a managed reranking model by **Cohere** that can be used to rerank documents. It is a transformer model that is trained on a large dataset of queries and documents to rerank documents based on their relevance to the query.\n",
    "\n",
    "The model includes\n",
    "- 4k context length to significantly improve search quality for longer documents \n",
    "- Ability to search over multi-aspect and semi-structured data like emails, invoices, JSON documents, code, and tables\n",
    "- Multilingual coverage of 100+ languages \n",
    "\n",
    "Since it is closed source we can not go through the inner workings of the model, but the in many applications it has shown to be very effective at reducing latency and increasing accuracy of the generation step. An open source alternative is [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) available at Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CohereRerank(api_key=COHERE_API_KEY, top_n=3, model=\"rerank-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "display(Markdown(f'Nodes:\\n\\n{\"\\n\\n---------------\\n\\n\".join([node.text for node in response.source_nodes])}'))\n",
    "display(Markdown(f'<b>{response}</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Compression\n",
    "\n",
    "Prompt compression is the process of reducing the length of the prompt to focus on the most important information. This can be useful when the prompt is too long or contains irrelevant information. Its also an effective way to reduce the computational cost (reducing time and money spent) of the model as well as combat the \"Lost in the middle\" phenomenon.\n",
    "\n",
    "In this example we will be using **LLMLingua** developed by Microsoft Research ([original paper](https://arxiv.org/pdf/2310.05736)) to reduce the size of prompts, while keeping the information that is relevant to the query.\n",
    "The main idea behind LLMLingua is to use a smaller language model to calculate the mutual information between the prompt and the query and use this to perform prompt compression.\n",
    "\n",
    "More specifically we will be using a process called **LongLLMLingua**. This process starts by reordering the documents to have the most relevant information at the top. Then it uses LLMLingua to compress the prompt and finally uses the compressed prompt to generate the final output.\n",
    "\n",
    "Other methods of prompt compression include:\n",
    "- [Selective Context](https://arxiv.org/pdf/2304.12102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_lingua_compressor = LongLLMLinguaPostprocessor(\n",
    "    instruction_str=\"Given the context, please answer the final question\",\n",
    "    target_token=300,\n",
    "    \n",
    "    rank_method=\"longllmlingua\",\n",
    "    additional_compress_kwargs={\n",
    "        \"condition_compare\": True,\n",
    "        \"condition_in_question\": \"after\",\n",
    "        \"context_budget\": \"+100\",\n",
    "        \"reorder_context\": \"sort\",  # enable document reorder\n",
    "        \"dynamic_context_compression_ratio\": 0.4, # enable dynamic compression ratio\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it the processor fails with \"CUDA\" error. You might need to make sure that pytorch is install with CUDA\n",
    "\n",
    "Check by running `torch.cuda.is_available()` and if it returns `True` then you have CUDA installed. If it returns `False` then you need to install pytorch with CUDA support.\n",
    "The following pip command will install the correct version of pytorch with CUDA support. (Versions may with computer and OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install torch==2.3.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install --force-reinstall Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results before and after compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retrieved_nodes = llm_lingua_compressor.postprocess_nodes(\n",
    "    retrieved_nodes, query_bundle=QueryBundle(query_str=query)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_contexts = \"\\n\\n\".join([n.get_content() for n in retrieved_nodes])\n",
    "compressed_contexts = \"\\n\\n\".join([n.get_content() for n in new_retrieved_nodes])\n",
    "\n",
    "original_tokens = llm_lingua_compressor._llm_lingua.get_token_length(original_contexts)\n",
    "compressed_tokens = llm_lingua_compressor._llm_lingua.get_token_length(compressed_contexts)\n",
    "\n",
    "print(\"Original Contexts:\")\n",
    "print(\"-------------------\")\n",
    "print(original_contexts)\n",
    "print(\"-------------------\")\n",
    "print(\"Compressed Contexts:\")\n",
    "print(\"-------------------\")\n",
    "print(compressed_contexts)\n",
    "print(\"-------------------\")\n",
    "print(\"Original Tokens:\", original_tokens)\n",
    "print(\"Compressed Tokens:\", compressed_tokens)\n",
    "print(\"Compressed Ratio:\", f\"{original_tokens/(compressed_tokens + 1e-5):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[llm_lingua_compressor],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
