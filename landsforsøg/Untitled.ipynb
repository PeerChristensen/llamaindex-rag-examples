{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e8ba18-41fb-4db2-a708-ee9138c290dd",
   "metadata": {},
   "source": [
    "# Basic RAG from existing index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f55ee0-e8b4-4f24-8ff1-6913db380c4a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87ac3b4f-d5ce-4c0b-a3a7-ccd5e37e1a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce3c65a9-d214-44da-a8cc-2b8b94094b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-05-01-preview\", # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    api_type=\"azure\",\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "chroma_client_load = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag/chromadb\",\n",
    "    settings=Settings(allow_reset=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c9ca2-7d39-4f90-ab03-cf02c71e38c9",
   "metadata": {},
   "source": [
    "## Load collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b6a74de-c528-4aa3-88bf-78a550ab681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_load = chroma_client_load.get_collection(name=\"landsforsoeg\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00279ca0-cecc-40fc-bfff-2dab69aa3045",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68f57092-5089-4d00-bec0-5d82269412e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that answers questions about the content of documents and provides detailed expert advice. \n",
    "    You must provide your answer in the Danish language.\n",
    "    If the answer contains multiple steps or points, provide the answer in a bullet format.\n",
    "    Below the answer, the source of the answer should be provided including file_name and page number.\n",
    "    ---------------------\n",
    "    {context}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {query}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88a527-857a-4df5-910e-97e1959e2e7c",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f9ad004-7f57-40b6-b7a2-5eea58147b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hvem udgiver landsforsøgene?\"\n",
    "#query = \"hvem udgiver landsforsøgene?\"\n",
    "#query = \"hvordan kan jeg bedst bekæmpe væselhale?\"\n",
    "query = \"hvordan vælger jeg den bedste vårbygsort?\"\n",
    "#query = \"hvad er reglerne for afstande ved etablering af husdyranlæg?\"\n",
    "query = \"Beskriv MT-Klovvask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5d35517-ede2-4210-af4b-83b8f8772ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection_load.query(query_texts=[query], n_results=5)\n",
    "context = result[\"documents\"][0]\n",
    "\n",
    "message = prompt.format(query=query, context=\"\\n\\n\".join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7cd27b04-28c5-46f7-a423-2af5be7838b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- MT-Klovvask består af en bund udformet af dørkplader i aluminium dækket med gummigulv og siderne er brune krydsfinerplader. \n",
       "- Sider er indsat for at forhindre køerne i at sidde fast eller skade benene ved at træde ud over siden. \n",
       "- Klovene spules bagfra med fire kraftige stråler af vand, som kan kombineres med spuling fra to dyser forfra.\n",
       "- MT-Klovvasken kan placeres før eller efter malkerområdet.\n",
       "- Det fungerer ved hjælp af koldt vand under tryk og spuler klovene med 5-7 liter vand i sekundet. \n",
       "- Vandet recirkulerer fra en tank under klovvaskeren og kan automatisk skiftes efter individuel tidsindstilling, og tanken kan spules ren. \n",
       "- Der er en sensor, der aktiverer klovvaskeren, lige før en ko går igennem, og den stopper 1,5 minutter efter sidste ko.\n",
       "\n",
       "Kilde: FarmTest Kvæg nr. 73, side 12."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai_client.chat.completions.create(\n",
    "    #messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    model=\"gpt4\",\n",
    "    messages=[\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
    "        for m in [{\"role\": \"user\", \"content\": message}]#st.session_state.messages\n",
    "    ],\n",
    "    stream=True)\n",
    "\n",
    "output = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices:  # Check if the list is not empty\n",
    "        output += chunk.choices[0].delta.content or \"\"\n",
    "    display(Markdown(f\"{output}\"), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7b972-1e58-4f6d-8ba3-79c455f7988d",
   "metadata": {},
   "source": [
    "## Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95b3db36-01d1-4b9f-a2d6-90e0b8ee7b45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKontekst:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m context_str\n\u001b[1;32m     21\u001b[0m synthesizer \u001b[38;5;241m=\u001b[39m get_response_synthesizer(response_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompact\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m RAGQueryEngine(\n\u001b[0;32m---> 23\u001b[0m     retriever\u001b[38;5;241m=\u001b[39m\u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[1;32m     24\u001b[0m     response_synthesizer\u001b[38;5;241m=\u001b[39msynthesizer,\n\u001b[1;32m     25\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     26\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "class RAGQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "    llm: AzureOpenAI\n",
    "    prompt: PromptTemplate\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content(metadata_mode=\"all\") for n in nodes])\n",
    "        #context = qa_prompt.format(\n",
    "        #    context_str=context_str, query_str=query_str)\n",
    "        response = self.llm.complete(\n",
    "            qprompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "                    \n",
    "        return str(response) + \"\\n\\n-------------------------\\n\\nKontekst:\\n\\n\" + context_str\n",
    "\n",
    "\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever=index.as_retriever(),\n",
    "    response_synthesizer=synthesizer,\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2694b8c-4cf0-4517-8a51-7dea1cdd7b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
