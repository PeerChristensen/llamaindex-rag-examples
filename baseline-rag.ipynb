{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcba9c38-ce3e-486a-8d3a-96e3a37b70ea",
   "metadata": {},
   "source": [
    "# Baseline RAG example\n",
    "\n",
    "This is a simple example of a baseline RAG application which purpose is to answer questions about the fantasy series [Malazan Universe](https://malazan.fandom.com/wiki/Malazan_Wiki) created by Steven Erikson and Ian C. Esslemont.\n",
    "\n",
    "First the example will show each step of a baseline RAG pipeline including **Indexing**, **Retrieval** and **Generation**. This is done in order to show the architecture without the abstraction provided by frameworks like LlamaIndex and LangChain.\n",
    "Then a more \"normal\" example will be shown using LlamaIndex.\n",
    "\n",
    "As a vector database, we will use [ChromaDB](https://docs.trychroma.com/), but this can easily be exchanged with other databases.\n",
    "\n",
    "In this example, we will use the following technologies\n",
    "\n",
    "- OpenAI API\n",
    "- ChromaDB\n",
    "- LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae217db9-221f-4811-9101-c87d3db2c821",
   "metadata": {},
   "source": [
    "### Setup libraries and environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf740a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install chromadb llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a26f5a-d799-4b80-93e5-60579a74852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb import Settings\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "import importlib\n",
    "import util\n",
    "\n",
    "#importlib.reload(util.helpers)\n",
    "from util.helpers import create_and_save_md_files, get_malazan_pages, get_office_pages, get_friends_pages, get_theoffice_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c0dca-fd51-4348-9ada-8ca12d0139bc",
   "metadata": {},
   "source": [
    "### Environment variables\n",
    "\n",
    "For this example you need to use an OpenAI API key. Go to [your API keys](https://platform.openai.com/api-keys) in the OpenAI console to generate one.\n",
    "\n",
    "Then add the following to a `.env` file in the root of the project.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b5d76d-dae0-4cfe-b9f1-1de2477b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1224aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=OPENAI_API_KEY,  \n",
    "    api_version=\"2024-05-01-preview\", # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ab2ea8-686e-427a-b512-3a829aa03d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    api_type=\"azure\",\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./landsforsøg/chromadb\", settings=Settings(allow_reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0eacc-5118-4dbb-a22f-63769c527184",
   "metadata": {},
   "source": [
    "## Fetch documents and save them as markdown files\n",
    "\n",
    "Here we fetch pages from the Fandom Malazan Wiki. These are the documents that we will use as our \"knowledge base\" in order to supply context to our prompts.\n",
    "\n",
    "We also pre-process the content in order to be able to add them to our vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72c1c81-1257-4e99-b252-f3315b868b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FandomPage 'Ross Geller'>, <FandomPage 'Chandler Bing'>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = get_friends_pages()\n",
    "pages\n",
    "#create_and_save_md_files(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9462c89b-2ab5-4172-8cad-ad1b8813517a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_and_save_md_files(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6f25f-66b0-4acb-8503-1b00c2868c20",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "In this step, we will index the documents in our vector database. This will allow us to retrieve the most relevant documents when we ask a question.\n",
    "\n",
    "We will use ChromaDB as our vector database and 'text-embedding-3-small' from OpenAI as our embedding model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8534742",
   "metadata": {},
   "source": [
    "#### Fetch and process saved documents\n",
    "\n",
    "First we need to fetch the documents we saved earlier.\n",
    "\n",
    "Then we will process the documents in order to add them to our vector database.\n",
    "The `SimpleDirectoryReader` fetches each section of the markdown file\n",
    "Then each section is split in to smaller chunks of text and each chunk is embedded using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add2504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 662 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('./landsforsøg/documents').load_data()\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "document_data = []\n",
    "\n",
    "for document in documents:\n",
    "    chunks = text_splitter.split_text(document.text)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=chunk, model=\"text-embedding-ada-002\")\n",
    "        document_data.append({\n",
    "            \"id\": f\"{document.id_}-{idx}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": document.metadata,\n",
    "            \"embedding\": embedding.data[0].embedding\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e5fcc",
   "metadata": {},
   "source": [
    "#### Add documents to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69311d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [doc[\"text\"] for doc in document_data]\n",
    "embeddings = [doc[\"embedding\"] for doc in document_data]\n",
    "metadatas = [doc[\"metadata\"] for doc in document_data]\n",
    "ids = [doc[\"id\"] for doc in document_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"landsforsoeg\", metadata={\"hnsw:space\": \"cosine\"}, embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d806a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9edc",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "In this step, we will retrieve the most relevant documents to a given question. We will use the vector database to retrieve the most similar documents to the question.\n",
    "\n",
    "In order to do this we will use the `text-embedding-3-small` model (**the same model used to index the documents**) from OpenAI to embed the question and then use the vector database to retrieve the most similar documents.\n",
    "\n",
    "We will retrieve the top 5 documents based on the _cosine similarity_ between the question and the documents. Other similarity metrics can be used as well like squared L2 or inner product.\n",
    "\n",
    "Change `cosine` to `l2` or `ip` when creating the collection above to try these out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2223843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hvordan bekæmper jeg væselhale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "156f5fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "TABEL 5.  Bekæmpelse af væselhale i rødsvingel 2023\n",
       "Rød-\n",
       "svingelBehandling l pr ha. Udbytte og \n",
       "merudbytte\n",
       "kg pr. ha Primo august Medio september\n",
       "Mateno \n",
       "DuoBoxerMateno \n",
       "DuoBoxer 2023\n",
       "Antal forsøg\n",
       " 1 1.449\n",
       " 2 0,35 60\n",
       " 3 0,7 142\n",
       " 4 0,35 11\n",
       " 5 0,7 -127\n",
       " 6 0,35 0,35 46\n",
       " 7 0,7 1 67\n",
       " 8 0,35 1 -50\n",
       " 9 0,35 0,5 0,35 0,5 -34\n",
       "10 0,35 1 0,35 1 -26\n",
       "LSD 109TABEL 6.  Tidlig Kerb 400 SC i strandsvingel\n",
       "Strand-\n",
       "svingelTilført Kerb \n",
       "400 SC\n",
       "l pr. haHerbicidskade forårUdbytte og merudbytte\n",
       "kg pr. ha\n",
       "Foder -\n",
       "typePlæne -\n",
       "typeBegge \n",
       "forsøgFoder -\n",
       "typePlæne -\n",
       "typeBegge \n",
       "forsøg\n",
       "Antal forsøg 1 1 2 1 1 2\n",
       "1 Ubehandlet 0 0 0,0 675 556 609\n",
       "2 0,1 0,2 3 1,6 633 449 543\n",
       "3 0,12 0,5 4 3,0 662 365 514\n",
       "4 0,15 0,5 6 4,6 596 380 488\n",
       "5 0,2 1 9 7,0 482 221 344\n",
       "LSD 48 48 49\n",
       "\n",
       "------------\n",
       "\n",
       "61 VINTERHVEDE UKRUDT\n",
       "reduktion i antal frøbærende rajgræsaks, som er resulta -\n",
       "tet af sen såning og god effekt af jordmidlerne Boxer og \n",
       "Mateno Duo. Det er en vigtig erkendelse ved håndtering \n",
       "af bestande, som har ALS-resistens eller metabolisk resi -\n",
       "stens.\n",
       "Bekæmpelse af væselhale om efteråret\n",
       "Der er udført tre forsøg i vinterhvede med bekæmpelse \n",
       "af væselhale med forskellige strategier med Boxer, Ma -\n",
       "teno Duo og Atlantis OD i henholdsvis stadie 10-11 og \n",
       "stadie 12. Behandlingerne ses i tabel 18.\n",
       "Forsøgene er udført på arealer med en meget stor be -\n",
       "stand af væselhale, i gennemsnit ca. 600 planter pr. m2 \n",
       "ved optælling i oktober. Den tidlige sprøjtning i stadie \n",
       "10-11 er udført fra 6 til 14 dage efter såning, som i gen -\n",
       "nemsnit har været midt i september. Anden sprøjtning i stadie 12 er udført 9 til 13 dage senere. Der har således \n",
       "været en god timing af behandlingerne, og der har været \n",
       "gode fugtighedsforhold til at jordmidlerne har kunnet \n",
       "virke optimalt.\n",
       "Effekten mod væselhale er bedømt ved optælling sidst i \n",
       "oktober og i november, dvs. henholdsvis en og to måne -\n",
       "der efter behandlingen i stadie 10-11. I november er der \n",
       "også udført en visuel bedømmelse af effekten.\n",
       "Forsøgsled 2 og 6 viser, at der er opnået samme effekt -\n",
       "niveau af 1,5 l Boxer pr. ha og 0,7 l Mateno Duo pr. ha.\n",
       "\n",
       "------------\n",
       "\n",
       "ha), 330 kg pr. ha NS 27-4, og 8 kg pr.ha Absolom rød -\n",
       "svingel (type med korte udløbere).\n",
       "Forsøget er anlagt i et design, der gør at alle behand -\n",
       "linger, udover etableringen, er de samme som den om -TABEL 10.  Bekæmpelse af væselhale i rødsvingel (J10)\n",
       "RødsvingelBehandlings-\n",
       "tidspunktHerbicid-\n",
       "skade1)\n",
       "3/4Væselhale \n",
       "pct. dækning \n",
       "af jord 3/4Væselhale \n",
       "pct. i frø ved \n",
       "høstUdb. og \n",
       "merudb., \n",
       "kg frø pr. haNettomer-\n",
       "udbytte, \n",
       "kg pr. ha\n",
       "2022. 2 forsøg\n",
       "1.1.\n",
       "\n",
       "------------\n",
       "\n",
       "Erfaringer viser, at det kan være svært at håndtere udendørs, når vejret begynder at blive køligere og mere fugtigt. \n",
       "Kan en vask af klovene reducere om -\n",
       "fanget af den konkrete klovsygdom?\n",
       "Ved investering i en klovvasker kan det være en god idé, at vide hvilke klovlidelser, der reelt er det store problem i besætnin -\n",
       "gen. Dette kan med fordel gøres ved at re -\n",
       "gistrere alle klovlidelserne ved minimum en klovbeskæring. \n",
       "Den hjemmelavede ”billige” klovva -\n",
       "skerløsning\n",
       "Det er en mulighed at få sin lokale smed til at lave en klovvasker, som spuler bagbe -\n",
       "\n",
       "------------\n",
       "\n",
       "ha og 40 kg fosfor pr. ha, er der i dette forsøg ikke \n",
       "høstes et merudbytte for at placere gødningen. Forsøget \n",
       "gentages i 2023.\n",
       "Rødsvingel\n",
       "Bekæmpelse af væselhale i rødsvingel\n",
       "I 2022 er der i samarbejde med DLF gennemført forsøg \n",
       "med at bekæmpe væselhale i rødsvingel . Der indgår to \n",
       "forsøg i denne forsøgsserie, som begge er udført i 1. års \n",
       "rødsvingel af en type med lange udløbere udlagt i vår -\n",
       "byg. Væselhale skal bekæmpes så tidligt som muligt, \n",
       "hvilket vil sige kort tid efter høst af dæksæd. For at få \n",
       "bedst mulig effekt af Mateno Duo 600 SC, er det afgø -\n",
       "rende, at der er en vis jordfugt til stede, hvilket kan være \n",
       "en udfordring i august måned. Som det fremgår af tabel \n",
       "10, er der ikke signifikante udbyttetab ved nogle af be -\n",
       "handlingerne, og der er effekt på indholdet af væselhale \n",
       "i frøanalysen ved behandling med Mateno Duo 600 SC. \n",
       "I det ubehandlede led er der et indhold på 0,15 procent \n",
       "væselhale i frøet, og i alle behandlingerne er dette redu -\n",
       "ceret. Bedste behandling er i led 3, hvor der er behand -\n",
       "let med 0,7 l. Mateno Duo 600 SC pr. ha den 20. august. \n",
       "Her er der ikke fundet væselhale i frøet, og der er et lille \n",
       "merudbytte for behandlingen, merudbyttet er dog ikke \n",
       "signifikant. Som det er set i andre forsøg med Mateno \n",
       "Duo 600 SC, giver kombinationen med Boxer en bedre \n",
       "bekæmpelse af væselhale."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = collection.query(query_texts=[query], n_results=5)\n",
    "context = result[\"documents\"][0]\n",
    "#display(Markdown(f\"------------\\n\\n{\"\\n\\n------------\\n\\n\".join(context)}\"))\n",
    "\n",
    "formatted_text = \"\\n\\n------------\\n\\n\".join(context)\n",
    "\n",
    "# Display the formatted markdown\n",
    "display(Markdown(f\"{formatted_text}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7a6a0",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "In this step, we will generate an answer to the question using the retrieved documents as context. We will use the OpenAI API to generate the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a1f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a helpful assistant that answers questions about the friends characters using provided context. \n",
       "\n",
       "Question: hvordan bekæmper jeg væselhale\n",
       "\n",
       "Context: \n",
       "\n",
       "-----------------------------------\n",
       "TABEL 5.  Bekæmpelse af væselhale i rødsvingel 2023\n",
       "Rød-\n",
       "svingelBehandling l pr ha. Udbytte og \n",
       "merudbytte\n",
       "kg pr. ha Primo august Medio september\n",
       "Mateno \n",
       "DuoBoxerMateno \n",
       "DuoBoxer 2023\n",
       "Antal forsøg\n",
       " 1 1.449\n",
       " 2 0,35 60\n",
       " 3 0,7 142\n",
       " 4 0,35 11\n",
       " 5 0,7 -127\n",
       " 6 0,35 0,35 46\n",
       " 7 0,7 1 67\n",
       " 8 0,35 1 -50\n",
       " 9 0,35 0,5 0,35 0,5 -34\n",
       "10 0,35 1 0,35 1 -26\n",
       "LSD 109TABEL 6.  Tidlig Kerb 400 SC i strandsvingel\n",
       "Strand-\n",
       "svingelTilført Kerb \n",
       "400 SC\n",
       "l pr. haHerbicidskade forårUdbytte og merudbytte\n",
       "kg pr. ha\n",
       "Foder -\n",
       "typePlæne -\n",
       "typeBegge \n",
       "forsøgFoder -\n",
       "typePlæne -\n",
       "typeBegge \n",
       "forsøg\n",
       "Antal forsøg 1 1 2 1 1 2\n",
       "1 Ubehandlet 0 0 0,0 675 556 609\n",
       "2 0,1 0,2 3 1,6 633 449 543\n",
       "3 0,12 0,5 4 3,0 662 365 514\n",
       "4 0,15 0,5 6 4,6 596 380 488\n",
       "5 0,2 1 9 7,0 482 221 344\n",
       "LSD 48 48 49\n",
       "\n",
       "61 VINTERHVEDE UKRUDT\n",
       "reduktion i antal frøbærende rajgræsaks, som er resulta -\n",
       "tet af sen såning og god effekt af jordmidlerne Boxer og \n",
       "Mateno Duo. Det er en vigtig erkendelse ved håndtering \n",
       "af bestande, som har ALS-resistens eller metabolisk resi -\n",
       "stens.\n",
       "Bekæmpelse af væselhale om efteråret\n",
       "Der er udført tre forsøg i vinterhvede med bekæmpelse \n",
       "af væselhale med forskellige strategier med Boxer, Ma -\n",
       "teno Duo og Atlantis OD i henholdsvis stadie 10-11 og \n",
       "stadie 12. Behandlingerne ses i tabel 18.\n",
       "Forsøgene er udført på arealer med en meget stor be -\n",
       "stand af væselhale, i gennemsnit ca. 600 planter pr. m2 \n",
       "ved optælling i oktober. Den tidlige sprøjtning i stadie \n",
       "10-11 er udført fra 6 til 14 dage efter såning, som i gen -\n",
       "nemsnit har været midt i september. Anden sprøjtning i stadie 12 er udført 9 til 13 dage senere. Der har således \n",
       "været en god timing af behandlingerne, og der har været \n",
       "gode fugtighedsforhold til at jordmidlerne har kunnet \n",
       "virke optimalt.\n",
       "Effekten mod væselhale er bedømt ved optælling sidst i \n",
       "oktober og i november, dvs. henholdsvis en og to måne -\n",
       "der efter behandlingen i stadie 10-11. I november er der \n",
       "også udført en visuel bedømmelse af effekten.\n",
       "Forsøgsled 2 og 6 viser, at der er opnået samme effekt -\n",
       "niveau af 1,5 l Boxer pr. ha og 0,7 l Mateno Duo pr. ha.\n",
       "\n",
       "ha), 330 kg pr. ha NS 27-4, og 8 kg pr.ha Absolom rød -\n",
       "svingel (type med korte udløbere).\n",
       "Forsøget er anlagt i et design, der gør at alle behand -\n",
       "linger, udover etableringen, er de samme som den om -TABEL 10.  Bekæmpelse af væselhale i rødsvingel (J10)\n",
       "RødsvingelBehandlings-\n",
       "tidspunktHerbicid-\n",
       "skade1)\n",
       "3/4Væselhale \n",
       "pct. dækning \n",
       "af jord 3/4Væselhale \n",
       "pct. i frø ved \n",
       "høstUdb. og \n",
       "merudb., \n",
       "kg frø pr. haNettomer-\n",
       "udbytte, \n",
       "kg pr. ha\n",
       "2022. 2 forsøg\n",
       "1.1.\n",
       "\n",
       "Erfaringer viser, at det kan være svært at håndtere udendørs, når vejret begynder at blive køligere og mere fugtigt. \n",
       "Kan en vask af klovene reducere om -\n",
       "fanget af den konkrete klovsygdom?\n",
       "Ved investering i en klovvasker kan det være en god idé, at vide hvilke klovlidelser, der reelt er det store problem i besætnin -\n",
       "gen. Dette kan med fordel gøres ved at re -\n",
       "gistrere alle klovlidelserne ved minimum en klovbeskæring. \n",
       "Den hjemmelavede ”billige” klovva -\n",
       "skerløsning\n",
       "Det er en mulighed at få sin lokale smed til at lave en klovvasker, som spuler bagbe -\n",
       "\n",
       "ha og 40 kg fosfor pr. ha, er der i dette forsøg ikke \n",
       "høstes et merudbytte for at placere gødningen. Forsøget \n",
       "gentages i 2023.\n",
       "Rødsvingel\n",
       "Bekæmpelse af væselhale i rødsvingel\n",
       "I 2022 er der i samarbejde med DLF gennemført forsøg \n",
       "med at bekæmpe væselhale i rødsvingel . Der indgår to \n",
       "forsøg i denne forsøgsserie, som begge er udført i 1. års \n",
       "rødsvingel af en type med lange udløbere udlagt i vår -\n",
       "byg. Væselhale skal bekæmpes så tidligt som muligt, \n",
       "hvilket vil sige kort tid efter høst af dæksæd. For at få \n",
       "bedst mulig effekt af Mateno Duo 600 SC, er det afgø -\n",
       "rende, at der er en vis jordfugt til stede, hvilket kan være \n",
       "en udfordring i august måned. Som det fremgår af tabel \n",
       "10, er der ikke signifikante udbyttetab ved nogle af be -\n",
       "handlingerne, og der er effekt på indholdet af væselhale \n",
       "i frøanalysen ved behandling med Mateno Duo 600 SC. \n",
       "I det ubehandlede led er der et indhold på 0,15 procent \n",
       "væselhale i frøet, og i alle behandlingerne er dette redu -\n",
       "ceret. Bedste behandling er i led 3, hvor der er behand -\n",
       "let med 0,7 l. Mateno Duo 600 SC pr. ha den 20. august. \n",
       "Her er der ikke fundet væselhale i frøet, og der er et lille \n",
       "merudbytte for behandlingen, merudbyttet er dog ikke \n",
       "signifikant. Som det er set i andre forsøg med Mateno \n",
       "Duo 600 SC, giver kombinationen med Boxer en bedre \n",
       "bekæmpelse af væselhale.\n",
       "\n",
       "-----------------------------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\"\"\"You are a helpful assistant that answers questions about the friends characters using provided context. \n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context: \n",
    "\n",
    "-----------------------------------\n",
    "{context}\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "\"\"\")\n",
    "message = prompt.format(query=query, context=\"\\n\\n\".join(context))\n",
    "display(Markdown(f\"{message}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d375966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Væselhale er en type insekt, der typisk trives i fugtige områder. Bekæmpelsen kan kræve flere metoder. \n",
       "\n",
       "1. Fjern fugt: Den første og vigtigste metode er at fjerne fugtkilder, da væselhale trives i fugtige omgivelser. Tjek for lækager i rør og reparer dem. Installer et affugtningssystem, hvis fugtigheden i dit hjem er høj.\n",
       "\n",
       "2. Rengøring: Rengør dit hjem grundigt, især mørke og fugtige områder som badeværelser, kældre og lofter. Væselhale klækker fra æg, der kan gemme sig i små sprækker og revner.\n",
       "\n",
       "3. Professionelle Insektmidler: Hvis problemet vedvarer, skal du overveje at bruge et insektmiddel, der er specielt designet til at bekæmpe væselhale. Følg alle sikkerhedsanvisningerne på etiketten.\n",
       "\n",
       "4. Brug af kiselgur: Dette er en naturlig substans, der kan hjælpe med at bekæmpe væselhale. Det er et fint pulver, der kan strøs omkring områder, hvor du har bemærket aktivitet.\n",
       "\n",
       "5. Professionel Bistand: Hvis væselhaleinfestationen ikke forsvinder på trods af dine bestræbelser, kan det være nødvendigt at ansætte en professionel skadedyrsbekæmper.\n",
       "\n",
       "Husk, det er nemmere at forebygge en infestation end at bekæmpe den. Hold dit hjem rent og tørt, og vær opmærksom på mulige tegn på infestation, såsom at se væselhale i hjemmet."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai_client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    model=\"gpt4\",\n",
    "    stream=True)\n",
    "\n",
    "output = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices:  # Check if the list is not empty\n",
    "        output += chunk.choices[0].delta.content or \"\"\n",
    "    display(Markdown(f\"{output}\"), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fad80",
   "metadata": {},
   "source": [
    "## Normal example using LlamaIndex\n",
    "\n",
    "In this example, we will use LlamaIndex to abstract the indexing and retrieval steps. This shows how easily the same pipeline can be implemented using LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17491aeb-09e3-401a-a052-f9add8b5b606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-azure-openai\n",
    "%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8f763a35-5ab8-4f6f-bffe-0db3a69dca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "#from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# ChromaDB Vector Store\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag/chromadb\", settings=Settings(allow_reset=True))\n",
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"landsforsoeg\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"gpt4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Define the ingestion pipeline to add documents to vector store\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        embedding,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Create index with the vector store and using the embedding model\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96d80b4d-d10b-40fc-b805-4600a0bd2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from saved\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"friends\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58348922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 662 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./landsforsøg/documents').load_data()\n",
    "\n",
    "# Run pipeline\n",
    "pipeline.run(documents=documents) # num_workers=4\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa9720",
   "metadata": {},
   "source": [
    "#### Create base QueryEngine from LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79604a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a4ed",
   "metadata": {},
   "source": [
    "#### Or alternatively, create a CustomQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9f60384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "\n",
    "from llama_index.core.postprocessor import TimeWeightedPostprocessor\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that answers questions about landsforsøgene using provided context.\n",
    "    Answers must be provided in the Danish language and in the form of a list if multiple steps or points are included in the answer.\n",
    "    Below the answer, relevant metadata, i.e. filenames and page numbers should be provided.\n",
    "    Example: Kilde: filename.pdf, side 60-62.\n",
    "\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {query_str}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "class RAGQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "    llm: OpenAI\n",
    "    qa_prompt: PromptTemplate\n",
    "    postprocessor: TimeWeightedPostprocessor\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content(metadata_mode=\"all\") for n in nodes])\n",
    "        #context = qa_prompt.format(\n",
    "        #    context_str=context_str, query_str=query_str)\n",
    "        response = self.llm.complete(\n",
    "            qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "                    \n",
    "        return str(response) + \"\\n\\n-------------------------\\n\\nKontekst:\\n\\n\" + context_str\n",
    "\n",
    "\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "node_postprocessor = TimeWeightedPostprocessor(\n",
    "    time_decay=0.5, time_access_refresh=False, top_k=1\n",
    ")\n",
    "\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever=index.as_retriever(), #similarity_top_k=2\n",
    "    response_synthesizer=synthesizer,\n",
    "    postprocessor = node_postprocessor,\n",
    "    llm=llm,\n",
    "    qa_prompt=qa_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hvordan bekæmper jeg væselhale?\"\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce99b7d",
   "metadata": {},
   "source": [
    "## Simplest RAG implementation using LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cceb724b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chandler starts off as a Data Processor, a job he dislikes. He later becomes a Processing Supervisor. His work involves \"Statistical Analysis and Data Reconfiguration,\" which is revealed when he quits his job in season 9. After quitting his job, he briefly runs a new branch of the company in Tulsa, Oklahoma, but eventually quits that too. Monica then helps him secure a job in advertising, starting as an unpaid intern and later becoming a junior copywriter. In an alternate reality storyline, Chandler quits his job and works as a freelance writer, specializing in comics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"gpt4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "\n",
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "# build VectorStoreIndex that takes care of chunking documents\n",
    "# and encoding chunks to embeddings for future retrieval\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "# The QueryEngine class is equipped with the generator\n",
    "# and facilitates the retrieval and generation steps\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Use your Default RAG\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e07acfb-ab58-4c06-83e8-bf5c5a96bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ross dated three women mentioned in the context: Rachel, Emily Waltham, and Elizabeth Stevens. The context also mentions that Ross slept with 14 women during the series, but their names are not provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"how many women did Ross date and what were their names?\"\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981c81e-3ccd-4c4c-9b00-57ee46f2ccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
