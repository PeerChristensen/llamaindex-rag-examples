{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcba9c38-ce3e-486a-8d3a-96e3a37b70ea",
   "metadata": {},
   "source": [
    "# Baseline RAG example\n",
    "\n",
    "This is a simple example of a baseline RAG application which purpose is to answer questions about the fantasy series [Malazan Universe](https://malazan.fandom.com/wiki/Malazan_Wiki) created by Steven Erikson and Ian C. Esslemont.\n",
    "\n",
    "First the example will show each step of a baseline RAG pipeline including **Indexing**, **Retrieval** and **Generation**. This is done in order to show the architecture without the abstraction provided by frameworks like LlamaIndex and LangChain.\n",
    "Then a more \"normal\" example will be shown using LlamaIndex.\n",
    "\n",
    "As a vector database, we will use [ChromaDB](https://docs.trychroma.com/), but this can easily be exchanged with other databases.\n",
    "\n",
    "In this example, we will use the following technologies\n",
    "\n",
    "- OpenAI API\n",
    "- ChromaDB\n",
    "- LlamaIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae217db9-221f-4811-9101-c87d3db2c821",
   "metadata": {},
   "source": [
    "### Setup libraries and environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edf740a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma in /usr/local/lib/python3.11/site-packages (0.1.8)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/site-packages (from chromadb) (2.5.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.26.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.8.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/site-packages (from chromadb) (3.10.3)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.11/site-packages (from llama-index-vector-stores-chroma) (0.10.43)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.25.1)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.1.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.6.2)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.6.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.31.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (10.2.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.16.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /usr/local/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.14.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.9.3)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Requirement already satisfied: httpcore in /usr/local/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.5.15)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.8.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.20.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a26f5a-d799-4b80-93e5-60579a74852e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_functions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01membedding_functions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb import Settings\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "import importlib\n",
    "import util\n",
    "\n",
    "#importlib.reload(util.helpers)\n",
    "from util.helpers import create_and_save_md_files, get_malazan_pages, get_office_pages, get_friends_pages, get_theoffice_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c0dca-fd51-4348-9ada-8ca12d0139bc",
   "metadata": {},
   "source": [
    "### Environment variables\n",
    "\n",
    "For this example you need to use an OpenAI API key. Go to [your API keys](https://platform.openai.com/api-keys) in the OpenAI console to generate one.\n",
    "\n",
    "Then add the following to a `.env` file in the root of the project.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79b5d76d-dae0-4cfe-b9f1-1de2477b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1224aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=OPENAI_API_KEY,  \n",
    "    api_version=\"2024-05-01-preview\", # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ab2ea8-686e-427a-b512-3a829aa03d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    api_type=\"azure\",\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag/chromadb\", settings=Settings(allow_reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0eacc-5118-4dbb-a22f-63769c527184",
   "metadata": {},
   "source": [
    "## Fetch documents and save them as markdown files\n",
    "\n",
    "Here we fetch pages from the Fandom Malazan Wiki. These are the documents that we will use as our \"knowledge base\" in order to supply context to our prompts.\n",
    "\n",
    "We also pre-process the content in order to be able to add them to our vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72c1c81-1257-4e99-b252-f3315b868b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FandomPage 'Ross Geller'>, <FandomPage 'Chandler Bing'>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = get_friends_pages()\n",
    "pages\n",
    "#create_and_save_md_files(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9462c89b-2ab5-4172-8cad-ad1b8813517a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_and_save_md_files(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6f25f-66b0-4acb-8503-1b00c2868c20",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "In this step, we will index the documents in our vector database. This will allow us to retrieve the most relevant documents when we ask a question.\n",
    "\n",
    "We will use ChromaDB as our vector database and 'text-embedding-3-small' from OpenAI as our embedding model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8534742",
   "metadata": {},
   "source": [
    "#### Fetch and process saved documents\n",
    "\n",
    "First we need to fetch the documents we saved earlier.\n",
    "\n",
    "Then we will process the documents in order to add them to our vector database.\n",
    "The `SimpleDirectoryReader` fetches each section of the markdown file\n",
    "Then each section is split in to smaller chunks of text and each chunk is embedded using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add2504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "document_data = []\n",
    "\n",
    "for document in documents:\n",
    "    chunks = text_splitter.split_text(document.text)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=chunk, model=\"text-embedding-ada-002\")\n",
    "        document_data.append({\n",
    "            \"id\": f\"{document.id_}-{idx}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": document.metadata,\n",
    "            \"embedding\": embedding.data[0].embedding\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e5fcc",
   "metadata": {},
   "source": [
    "#### Add documents to ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69311d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [doc[\"text\"] for doc in document_data]\n",
    "embeddings = [doc[\"embedding\"] for doc in document_data]\n",
    "metadatas = [doc[\"metadata\"] for doc in document_data]\n",
    "ids = [doc[\"id\"] for doc in document_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"friends\", metadata={\"hnsw:space\": \"cosine\"}, embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d806a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9edc",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "In this step, we will retrieve the most relevant documents to a given question. We will use the vector database to retrieve the most similar documents to the question.\n",
    "\n",
    "In order to do this we will use the `text-embedding-3-small` model (**the same model used to index the documents**) from OpenAI to embed the question and then use the vector database to retrieve the most similar documents.\n",
    "\n",
    "We will retrieve the top 5 documents based on the _cosine similarity_ between the question and the documents. Other similarity metrics can be used as well like squared L2 or inner product.\n",
    "\n",
    "Change `cosine` to `l2` or `ip` when creating the collection above to try these out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2223843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Chandler's job?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.query(query_texts=[query], n_results=5)\n",
    "context = result[\"documents\"][0]\n",
    "#display(Markdown(f\"------------\\n\\n{\"\\n\\n------------\\n\\n\".join(context)}\"))\n",
    "\n",
    "formatted_text = \"\\n\\n------------\\n\\n\".join(context)\n",
    "\n",
    "# Display the formatted markdown\n",
    "display(Markdown(f\"{formatted_text}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7a6a0",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "In this step, we will generate an answer to the question using the retrieved documents as context. We will use the OpenAI API to generate the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a1f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a helpful assistant that answers questions about the friends characters using provided context. \n",
       "\n",
       "Question: What is Chandler's job?\n",
       "\n",
       "Context: \n",
       "\n",
       "-----------------------------------\n",
       "Much to Chandler's dismay, the job is as an unpaid intern. He soon secures a full-time paying job in the business as a junior copywriter where the internship was originally just for three assistants, as the company felt that Chandler was the strongest candidate in the program but was too experienced and mature to be just an assistant.\n",
       "In an alternate reality storyline during the show (\"The One That Could Have Been\"), Chandler has the guts to quit his job and works as a freelance writer, specializing in comics.\n",
       "\n",
       "Chandler Bing\n",
       "=============\n",
       "\n",
       "Career\n",
       "\n",
       "\n",
       "When the series begins, Chandler works as a Data Processor. This is a job which he thoroughly loathes, and tries to quit early in season 1 (\"The One With The Stoned Guy\"). His boss offers a promotion and more and more money to lure him back. According to the nameplate on his door (seen in \"The One With The Ick Factor\") he has become Processing Supervisor. Although never explicitly stated, exterior transition shots show that Chandler's office is in the Solow Building.\n",
       "It remains a running joke through most of the seasons that no one quite knows what he does.\n",
       "The guys win the trivia game because Rachel and Monica don't know what Chandler does for a living (\"The One With The Embryos\") (Although since Ross created the game, this suggests that he knows the answer)\n",
       "Joey once says, \" And you call yourself an accountant?\" and Chandler replies, \"No!\"\n",
       "Monica admits several times that she doesn't pay attention when he talks about his work, but finally learns what he does when he quits in season 9. She calls it, \"Statistical Analysis and Data Reconfiguration.\" Chandler responds by looking at her and saying, \"I quit, and you learn what I do?\"\n",
       "Chandler mentions his job title in, \"The One with the Cooking Class.\" This job inspires his oft-referenced office slang word \"WENUS\" (Weekly Estimated Net Usage Systems), as well as the \"ANUS\" (Annual Net Usage Statistics). Because of this job Chandler appears to be the most financially well-off among the six friends for the most part of the series and is also shown to hold a position of authority in his company.\n",
       "For a time, Chandler is unable to simply quit his job as it is his nature to avoid ending anything forcefully. This leads to an awkward moment where he dozes off during a crucial meeting and unintentionally agrees to run a new branch of the company in Tulsa, Oklahoma. Although he is able to work out a schedule that allows him to spend a few days each week in New York with Monica when she gets a new job at a restaurant in the city, when his new job forces him to work on Christmas Day, Chandler quits his job so he can fly home to be with Monica (\"The One With Christmas In Tulsa\").\n",
       "Monica helps Chandler secure a job in advertising. Much to Chandler's dismay, the job is as an unpaid intern.\n",
       "\n",
       "Personality\n",
       "\n",
       "\n",
       "\"Hi, I'm Chandler, I make jokes when I'm uncomfortable.\"\n",
       "—Chandler\n",
       "Though Chandler never lets up by using sarcasm as a defense, he has a tendency to come off as needy and makes bad first impressions as said by Phoebe with his constant joke-making and snarky demeanor. Despite this emotional immaturity, Chandler is the most financially secure of his friends.\n",
       "Chandler is dorky, quirky and estranged from both of his parents. He suffers from commitment issues, brought on by growing up in a broken home with no idea of what a stable marriage looks like, can be neurotic and extremely defensive with humor as his shield but his sense of humor is generally unsophisticated, to the point during an interview when his boss told him he'd have \"extra duties on his hand\" he had to stop himself from laughing. Chandler also associates everything that links to his parents' divorce in a negative light, specifically Thanksgiving where his parents reveal their separation over turkey where his father plans to run away with the pool boy. This also associates with his mistrust of people in adult relationships. Becoming extremely paranoid when his girlfriend, Kathy (who was an actress) shared a sex scene with her co-star prompting a big fight between the two. Ironically it was Chandler's own paranoia that drove Kathy to have an affair with the same person.\n",
       "His commitment issues are not only a running gag in the series but as well with his friends who use it as an excuse to mock him. In \"The One With The Lesbian Wedding\", when asked who of the group will get married last they all pointed at Chandler. This could either be a joke, a ribbing on his commitment-phobia or him being the least desirable of the group. However, he was the first friend to happily settle down as a married man. Even when he was a happily married man, Chandler still retained some of his paranoia when it was revealed that their surrogate mother was having twins, he instantly panicked and suggested keeping only one. However, when he discovered they were a male and a female, he was overjoyed.\n",
       "Chandler is relatively comfortable with complimenting looks of the same gender like Phoebe. However, he is quite humiliated by his effeminate nature and is a profound heterosexual whereas Phoebe has been hinted to be bisexual. It's revealed Joey is who he would go out with.\n",
       "\n",
       "Chandler Bing\n",
       "\n",
       "\n",
       "“\n",
       "I'm not great at the advice, can I interest you in a sarcastic comment?\n",
       "”\n",
       "—Chandler after being asked for advice.\n",
       "Chandler Muriel Bing is one of the main characters on the popular sitcom Friends (1994–2004), portrayed by the late Matthew Perry. Chandler has a very good sense of humor, and is notoriously sarcastic, an attribute he calls a defense mechanism he developed due to his parents' divorcing when he was a child. His best friends are Joey Tribbiani and Ross Geller. Chandler has a habit of asking rhetorical questions that start with \"Could it be any more...?\" or something similar, such as \"Could this be any more...?\" or \"Could she be any more...?\".\n",
       "\n",
       "-----------------------------------\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\"\"\"You are a helpful assistant that answers questions about the friends characters using provided context. \n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context: \n",
    "\n",
    "-----------------------------------\n",
    "{context}\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "\"\"\")\n",
    "message = prompt.format(query=query, context=\"\\n\\n\".join(context))\n",
    "display(Markdown(f\"{message}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d375966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the television show \"Friends\", Chandler Bing works in statistical analysis and data reconfiguration for a large multinational corporation. However, his job is often a source of humor among his friends who don't fully understand what he does. Later in the series, Chandler changes his career to advertising."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai_client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    model=\"gpt4\",\n",
    "    stream=True)\n",
    "\n",
    "output = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices:  # Check if the list is not empty\n",
    "        output += chunk.choices[0].delta.content or \"\"\n",
    "    display(Markdown(f\"{output}\"), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fad80",
   "metadata": {},
   "source": [
    "## Normal example using LlamaIndex\n",
    "\n",
    "In this example, we will use LlamaIndex to abstract the indexing and retrieval steps. This shows how easily the same pipeline can be implemented using LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17491aeb-09e3-401a-a052-f9add8b5b606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install llama-index-embeddings-azure-openai\n",
    "#%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f763a35-5ab8-4f6f-bffe-0db3a69dca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "# ChromaDB Vector Store\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./data/baseline-rag/chromadb\", settings=Settings(allow_reset=True))\n",
    "chroma_client.reset()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"friends\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"gpt4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Define the ingestion pipeline to add documents to vector store\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "        embedding,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Create index with the vector store and using the embedding model\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58348922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "# Run pipeline\n",
    "pipeline.run(documents=documents)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa9720",
   "metadata": {},
   "source": [
    "#### Create base QueryEngine from LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79604a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm, verbose=True)query_engine = index.as_query_engine(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a4ed",
   "metadata": {},
   "source": [
    "#### Or alternatively, create a CustomQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f60384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant that answers questions about the Malazan Fantasy Universe using provided context.\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {query_str}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "class RAGQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "    llm: OpenAI\n",
    "    qa_prompt: PromptTemplate\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "        print(\"Prompt:\\n\\n\", qa_prompt.format(\n",
    "            context_str=context_str, query_str=query_str))\n",
    "        response = self.llm.complete(\n",
    "            qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "\n",
    "        return str(response)\n",
    "\n",
    "\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "query_engine = RAGQueryEngine(\n",
    "    retriever=index.as_retriever(),\n",
    "    response_synthesizer=synthesizer,\n",
    "    llm=llm,\n",
    "    qa_prompt=qa_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab9d8ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chandler's job changes throughout the series. Initially, he works as a Data Processor and later becomes a Processing Supervisor. His job involves \"Statistical Analysis and Data Reconfiguration.\" Later on, he unintentionally agrees to run a new branch of the company in Tulsa, Oklahoma. However, he quits this job to be with Monica on Christmas Day. Eventually, Monica helps him secure a job in advertising, where he starts as an unpaid intern and later becomes a junior copywriter. In an alternate reality storyline, Chandler works as a freelance writer, specializing in comics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce99b7d",
   "metadata": {},
   "source": [
    "## Simplest RAG implementation using LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cceb724b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chandler starts off as a Data Processor, a job he dislikes. He later becomes a Processing Supervisor. His work involves \"Statistical Analysis and Data Reconfiguration,\" which is revealed when he quits his job in season 9. After quitting his job, he briefly runs a new branch of the company in Tulsa, Oklahoma, but eventually quits that too. Monica then helps him secure a job in advertising, starting as an unpaid intern and later becoming a junior copywriter. In an alternate reality storyline, Chandler quits his job and works as a freelance writer, specializing in comics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"gpt4\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"), # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "\n",
    "# Fetch documents\n",
    "documents = SimpleDirectoryReader('./data/docs').load_data()\n",
    "\n",
    "# build VectorStoreIndex that takes care of chunking documents\n",
    "# and encoding chunks to embeddings for future retrieval\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "# The QueryEngine class is equipped with the generator\n",
    "# and facilitates the retrieval and generation steps\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Use your Default RAG\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e07acfb-ab58-4c06-83e8-bf5c5a96bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ross dated three women mentioned in the context: Rachel, Emily Waltham, and Elizabeth Stevens. The context also mentions that Ross slept with 14 women during the series, but their names are not provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"how many women did Ross date and what were their names?\"\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981c81e-3ccd-4c4c-9b00-57ee46f2ccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
