{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk optimization\n",
    "\n",
    "I order to abide by a the context window of LLMs, documents are usually split into smaller parts when creating RAG pipelines. This is called chunking. While chunking comes with the added benefits of reducing costs and noise in the *generation* step, it also introduces a new problem: \"How do we prevent losing important information when splitting the document into chunks?\"\n",
    "\n",
    "In baseline RAG, we usually split the document into chunks of fixed size including a fixed overlap between adjacent chunks. In most common cases this practice works well and it is computationally efficient and does not require any NLP models.\n",
    "\n",
    "This notebook explores the problem of chunk optimization by exploring a few different strategies:\n",
    "\n",
    "1. **Fixed size chunking**: Split the document into chunks of fixed size.\n",
    "2. **Semantic chunking**: Considers the semantic meaning behind the text and divides the document into meaningful semantic chunks\n",
    "3. **Hyperparameter tuning**: Traditional ML via grid-search\n",
    "\n",
    "Other strategies include\n",
    "\n",
    "1. **Document Specific Chunking**: Split the document based on the logical sections of the document. Useful for Markdown, HTML, etc.\n",
    "2. **Recursive Chunking**: Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators. If the initial attempt at splitting the text doesnâ€™t produce chunks of the desired size or structure, the method recursively calls itself on the resulting chunks with a different separator or criterion until the desired chunk size or structure is achieved. \n",
    "3. **Agentic Chunk**: Use LLMs as \"agents\" and split the document into chunks in the fasion a human would do - start at the top and continue down the document while deciding whether to start a new chunk given the current sentence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from util.helpers import get_wiki_pages, create_and_save_wiki_md_files, pretty_print_node\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following to a `.env` file in the root of the project if not already there.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR_KEY_HERE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = get_wiki_pages([\"Vincent Van Gogh\"])\n",
    "create_and_save_wiki_md_files(pages=pages, path=\"./data/docs/wiki/\")\n",
    "documents = SimpleDirectoryReader(\"./data/docs/wiki/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbedding(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed size chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_size_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=40)\n",
    "fixed_nodes = fixed_size_splitter.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f'{\"\\n\\n------------\\n\\n\".join([node.get_content() for node in fixed_nodes[2:6]])}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_index = VectorStoreIndex(nodes=fixed_nodes)\n",
    "fixed_query_engine = fixed_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embedding)\n",
    "semantic_nodes = semantic_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f'{\"\\n\\n------------\\n\\n\".join([node.get_content() for node in semantic_nodes[3:7]])}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_index = VectorStoreIndex(nodes=semantic_nodes)\n",
    "semantic_query_engine = semantic_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the different chunking strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about Vincent Van Gogh's early life\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_retriever = fixed_index.as_retriever()\n",
    "fixed_retrieved_nodes = fixed_retriever.retrieve(query)\n",
    "pretty_print_node(fixed_retrieved_nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_index.as_retriever()\n",
    "semantic_retrieved_nodes = semantic_retriever.retrieve(query)\n",
    "pretty_print_node(semantic_retrieved_nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_response = fixed_query_engine.query(\n",
    "    query\n",
    ")\n",
    "print(str(fixed_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_response = semantic_query_engine.query(\n",
    "    query\n",
    ")\n",
    "print(str(semantic_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
